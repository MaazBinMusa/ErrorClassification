{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEsting fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "# Example: Load your dataset with tex col\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "def convert_labels(example):\n",
    "    example['labels'] = example['label']  # Assuming your label column is 'label'\n",
    "    return example\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(convert_labels)\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "# find total number of classes\n",
    "num_labels = len(set(train_dataset['labels']))\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"../data/induced_errors_v1.pkl\", \"rb\") as f:\n",
    "    df = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the col called \"old_text\"\n",
    "df = df.drop(columns=[\"old_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "error_name\n",
       "    12665\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter rows with distance 1.0 and error_name \"transposition\"\n",
    "df[(df[\"distance\"] == 1.0)][\"error_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Flava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/workspace1/ErrorClass/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Embedding Shape: torch.Size([1, 128, 768])\n",
      "Image Embedding Shape: torch.Size([1, 197, 768])\n",
      "Multimodal Embedding Shape: torch.Size([1, 326, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/workspace1/ErrorClass/lib/python3.12/site-packages/transformers/modeling_utils.py:1126: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import FlavaProcessor, FlavaModel\n",
    "from PIL import Image\n",
    "import urllib.request\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Load pre-trained model and processor\n",
    "processor = FlavaProcessor.from_pretrained(\"facebook/flava-full\")\n",
    "model = FlavaModel.from_pretrained(\"facebook/flava-full\")\n",
    "\n",
    "# Prepare inputs\n",
    "text = \"GG\"\n",
    "url = \"https://media.geeksforgeeks.org/wp-content/uploads/20210224040124/JSBinCollaborativeJavaScriptDebugging6-300x160.png\" \n",
    "urllib.request.urlretrieve(url, \"geeksforgeeks.png\")\n",
    "image = Image.open(r\"geeksforgeeks.png\").convert('RGB')\n",
    "\n",
    "\n",
    "inputs = processor(text=[text], images=image, return_tensors=\"pt\", max_length=128, padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Generate embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Access different types of embeddings\n",
    "text_embeddings = outputs.text_embeddings\n",
    "image_embeddings = outputs.image_embeddings\n",
    "multimodal_embeddings = outputs.multimodal_embeddings\n",
    "\n",
    "print(f\"Text Embedding Shape: {text_embeddings.shape}\")\n",
    "print(f\"Image Embedding Shape: {image_embeddings.shape}\")\n",
    "print(f\"Multimodal Embedding Shape: {multimodal_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128, 768]),\n",
       " torch.Size([1, 197, 768]),\n",
       " torch.Size([1, 326, 768]),\n",
       " odict_keys(['image_embeddings', 'image_output', 'text_embeddings', 'text_output', 'multimodal_embeddings', 'multimodal_output']))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeddings.shape, image_embeddings.shape, multimodal_embeddings.shape, outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing PHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flash-attn transformers torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "total images must be the same as the number of image tags, got 0 image tags and 1 images",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://static.startuptalky.com/2021/06/GeeksforGeeks-StartupTalky.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlretrieve(url, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmoco.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m extracted_text \u001b[38;5;241m=\u001b[39m \u001b[43mperform_ocr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmoco.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracted text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextracted_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m, in \u001b[0;36mperform_ocr\u001b[0;34m(image_path, prompt)\u001b[0m\n\u001b[1;32m     16\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m <image>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Process the inputs\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     22\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-vision-128k-instruct/c45209e90a4c4f7d16b2e9d48503c7f3e83623ed/processing_phi3_v.py:116\u001b[0m, in \u001b[0;36mPhi3VProcessor.__call__\u001b[0;34m(self, text, images, padding, truncation, max_length, return_tensors)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     image_inputs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 116\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_images_texts_to_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-vision-128k-instruct/c45209e90a4c4f7d16b2e9d48503c7f3e83623ed/processing_phi3_v.py:174\u001b[0m, in \u001b[0;36mPhi3VProcessor._convert_images_texts_to_inputs\u001b[0;34m(self, images, texts, padding, truncation, max_length, return_tensors)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m unique_image_ids \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(unique_image_ids)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_ids must start from 1, and must be continuous int, e.g. [1, 2, 3], cannot be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munique_image_ids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# total images must be the same as the number of image tags\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_image_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(images), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal images must be the same as the number of image tags, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(unique_image_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m image tags and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m images\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    176\u001b[0m image_ids_pad \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m-\u001b[39miid]\u001b[38;5;241m*\u001b[39mnum_img_tokens[iid\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m iid \u001b[38;5;129;01min\u001b[39;00m image_ids]\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minsert_separator\u001b[39m(X, sep_list):\n",
      "\u001b[0;31mAssertionError\u001b[0m: total images must be the same as the number of image tags, got 0 image tags and 1 images"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM # Load AutoModelForCausalLM instead of AutoModelForVision2Seq\n",
    "from PIL import Image\n",
    "import torch\n",
    "import urllib.request\n",
    "\n",
    "# Load model and processor\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/Phi-3-vision-128k-instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-vision-128k-instruct\",trust_remote_code=True) # Load the model using AutoModelForCausalLM\n",
    "\n",
    "# Function to perform OCR on a single image\n",
    "def perform_ocr(image_path, prompt=\"Perform OCR on this image:\"):\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Create the input text with an image tag\n",
    "    input_text = f\"{prompt} <image>\"\n",
    "    \n",
    "    # Process the inputs\n",
    "    inputs = processor(text=input_text, images=image, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "    \n",
    "    text = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "url = \"https://static.startuptalky.com/2021/06/GeeksforGeeks-StartupTalky.jpg\"\n",
    "urllib.request.urlretrieve(url, \"moco.png\")\n",
    "extracted_text = perform_ocr(\"moco.png\")\n",
    "print(f\"Extracted text: {extracted_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ErrorClass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
